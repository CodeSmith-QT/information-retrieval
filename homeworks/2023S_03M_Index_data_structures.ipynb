{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0c8bff7-32ef-4f8c-879a-1a16dbd26602"
      },
      "source": [
        "# This file contains master-level homework on index structures\n",
        "\n",
        "You know, that you can implement an arbitrary metric? To get full points your task will be the following:\n",
        "\n",
        "1. Choose a small sample of the original data (e.g. 10K New York points).\n",
        "2. Describe each point with coordinate vector and discrete set of words in the name. **Design the metric function** -- a function, which accepts 2 objects and returns a number. This function should esimate \"distance\" in a merged space of words and distances. No common words? Far. Both common words and vectors are similar? Close!\n",
        "3. **Use this metric in index data structure**. Maybe you will [extend nmslib](https://github.com/nmslib/nmslib/issues/478), maybe you will prefer my [NSW implementation](https://github.com/IUCVLab/proximity-cut/blob/master/modules/nsw/nsw.py) ([usage](https://github.com/IUCVLab/proximity-cut/blob/master/tests/nsw-visualization.ipynb), [custom HVDM metric](https://github.com/IUCVLab/proximity-cut/blob/master/modules/tools/hvdm.py), custom metric application -- pass `dist=func` into constructor). Or maybe you will find a data structure which supports this from the box :)\n",
        "4. Run some tests!"
      ],
      "id": "f0c8bff7-32ef-4f8c-879a-1a16dbd26602"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "f8f0bb48-97e7-4a59-a3bf-9a25081a52da"
      },
      "outputs": [],
      "source": [
        "# copy all necessary code from another file here"
      ],
      "id": "f8f0bb48-97e7-4a59-a3bf-9a25081a52da"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "B3eTQYbPTPsa",
        "outputId": "0352b1fb-9497-4de3-b078-60d480dda382"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1LUudtCADqSxRl18ZzCzyPPGfhuUo2ZZs\n",
            "To: /content/poi_sample_2M.zip\n",
            "100%|██████████| 105M/105M [00:00<00:00, 126MB/s] \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'poi_sample_2M.zip'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "import gdown\n",
        "url = 'https://drive.google.com/uc?id=1LUudtCADqSxRl18ZzCzyPPGfhuUo2ZZs'\n",
        "output = 'poi_sample_2M.zip'\n",
        "gdown.download(url, output, quiet=False)"
      ],
      "id": "B3eTQYbPTPsa"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "yyDde3I4TRs7"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "z = zipfile.ZipFile('poi_sample_2M.zip')\n",
        "z.extractall()"
      ],
      "id": "yyDde3I4TRs7"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "4GVLT2wLTTgW"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def split_shards(file, folder='shard', capacity=20000):\n",
        "    import pickle, os, math, gc\n",
        "    if not os.path.exists(folder):\n",
        "        os.mkdir(folder)\n",
        "    with open(file, \"rb\") as f:\n",
        "        dataset = pickle.load(f)\n",
        "    nshards = len(dataset) // capacity\n",
        "    if nshards * capacity < len(dataset):\n",
        "        nshards += 1\n",
        "    \n",
        "    for i in range(nshards):\n",
        "        with open(f\"{folder}/{i}\", 'wb') as f:\n",
        "            part = dataset[i * capacity:(i+1)*capacity]\n",
        "            pickle.dump(part, f)\n",
        "    dataset = None\n",
        "    gc.collect()            \n",
        "\n",
        "    \n",
        "def dataset_get(indices, folder='shard', capacity=20000) -> list:\n",
        "    result = []\n",
        "    groups = {}\n",
        "    for i in indices:\n",
        "        x = i // capacity\n",
        "        if x not in groups:\n",
        "            groups[x] = []\n",
        "        groups[x].append(i)\n",
        "    for x in groups:\n",
        "        with open(f\"{folder}/{int(x)}\", \"rb\") as f:\n",
        "            sha = pickle.load(f)\n",
        "            for i in groups[x]:\n",
        "                row = sha[int(i % capacity)]\n",
        "                result.append(row)\n",
        "    return result\n",
        "\n",
        "\n",
        "# should return iterator, which goes through all elements, consequently opening files\n",
        "# use ``yield`` operator to simplify your code\n",
        "def iterate_dataset(items, folder=\"shard\", capacity=20000):\n",
        "    # TODO write your code instead\n",
        "    result = []\n",
        "    groups = {}\n",
        "    for i in range(items):\n",
        "        x = i // capacity\n",
        "        if x not in groups:\n",
        "            groups[x] = []\n",
        "        groups[x].append(i)\n",
        "    for x in groups:\n",
        "        with open(f\"{folder}/{x}\", \"rb\") as f:\n",
        "            sha = pickle.load(f)\n",
        "            for i in groups[x]:\n",
        "                row = sha[i % capacity]\n",
        "                yield row    "
      ],
      "id": "4GVLT2wLTTgW"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "uwsDrRbcT1og"
      },
      "outputs": [],
      "source": [
        "split_shards(\"poi_sample01.pickle\")"
      ],
      "id": "uwsDrRbcT1og"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "J__i9iduVpsf"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "# this function returns a pair of tuples: NE and SW corners.\n",
        "def get_town_range_coordinates(town: str, google_maps_api_key: str) -> tuple:\n",
        "  api = f\"https://geocode-maps.yandex.ru/1.x/?format=json&apikey={google_maps_api_key}&geocode={town}\"\n",
        "  response = requests.get(api)\n",
        "  temp = response.json()['response'][\"GeoObjectCollection\"]['featureMember'][0][\"GeoObject\"][\"boundedBy\"]['Envelope']\n",
        "  sw = [float(x.strip()) for x in temp['lowerCorner'].split()]\n",
        "  ne = [float(x.strip()) for x in temp['upperCorner'].split()]\n",
        "  return ne, sw\n",
        "\n",
        "GEO_CACHE = {}\n",
        "def get_town_range_coordinates_cached(town: str, maps_key: str) -> tuple:\n",
        "    global GEO_CACHE\n",
        "    if GEO_CACHE is None:\n",
        "      GEO_CACHE = {}\n",
        "    if town not in GEO_CACHE:      \n",
        "      GEO_CACHE[town] = get_town_range_coordinates(town, maps_key)\n",
        "    else: \n",
        "      print('Cache Hit!')\n",
        "    return GEO_CACHE[town]"
      ],
      "id": "J__i9iduVpsf"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Replace the api key with a yandex api key"
      ],
      "metadata": {
        "id": "I-ol9lUvN3Vl"
      },
      "id": "I-ol9lUvN3Vl"
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "BgbnnmF_VsKr"
      },
      "outputs": [],
      "source": [
        "my_google_maps_api_key = None"
      ],
      "id": "BgbnnmF_VsKr"
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ET6MhoJbJhs",
        "outputId": "584b859d-900d-4605-9fb9-d043f4feb643"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))"
      ],
      "id": "8ET6MhoJbJhs"
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vokYtxGLcvyb",
        "outputId": "d3129b17-2d1c-4e9d-ad6d-95c73c5b3369"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.9/dist-packages (3.5.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.9/dist-packages (from spacy) (1.22.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from spacy) (3.1.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.9/dist-packages (from spacy) (1.0.9)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.9/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.9/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.9/dist-packages (from spacy) (8.1.9)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from spacy) (3.0.8)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.9/dist-packages (from spacy) (1.10.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from spacy) (23.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.9/dist-packages (from spacy) (2.0.7)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.9/dist-packages (from spacy) (0.7.0)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from spacy) (1.0.4)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.9/dist-packages (from spacy) (0.10.1)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.9/dist-packages (from spacy) (6.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from spacy) (67.6.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.9/dist-packages (from spacy) (2.27.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.9/dist-packages (from spacy) (4.65.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.9/dist-packages (from spacy) (2.4.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.9/dist-packages (from spacy) (1.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.9/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.15)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.9/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.0.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.9/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.9)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.9/dist-packages (from typer<0.8.0,>=0.3.0->spacy) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->spacy) (2.1.2)\n",
            "2023-04-09 10:18:35.559147: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en-core-web-md==3.5.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.5.0/en_core_web_md-3.5.0-py3-none-any.whl (42.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 MB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /usr/local/lib/python3.9/dist-packages (from en-core-web-md==3.5.0) (3.5.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (67.6.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (3.0.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.0.8)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.22.4)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.1.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.10.7)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (8.1.9)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (3.3.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (0.10.1)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (0.7.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.0.9)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (3.0.12)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (3.1.2)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.0.4)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.4.6)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (6.3.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.27.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.0.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (23.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (4.65.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.9/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (4.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.0.12)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.9/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (0.0.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.9/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (0.7.9)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.9/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_md')\n"
          ]
        }
      ],
      "source": [
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_md"
      ],
      "id": "vokYtxGLcvyb"
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "KhoWEIQrcyhn"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_md')"
      ],
      "id": "KhoWEIQrcyhn"
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "fqb07Y45a9pS"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import word_tokenize \n",
        "import numpy as np\n",
        "\n",
        "def clean_word(sentence):\n",
        "  temp = set(token.lower().strip()\n",
        "    for token in word_tokenize(sentence)\n",
        "    if token.isalpha() and token.lower() not in stop_words and\n",
        "    token.lower() in nlp.vocab.strings)  \n",
        "  if len(temp) > 0:\n",
        "    return np.mean([nlp.vocab[token].vector for token in temp], axis=0)  "
      ],
      "id": "fqb07Y45a9pS"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c03b24cb-9be8-44e4-bc45-2042891b624b"
      },
      "source": [
        "## 1. Sample some data"
      ],
      "id": "c03b24cb-9be8-44e4-bc45-2042891b624b"
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "9ptY2CtAVtG9"
      },
      "outputs": [],
      "source": [
        "ne_bound, sw_bound = get_town_range_coordinates_cached('New york', my_google_maps_api_key)"
      ],
      "id": "9ptY2CtAVtG9"
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "7anEzodeUnI-"
      },
      "outputs": [],
      "source": [
        "datasets = []\n",
        "for i,item in enumerate(iterate_dataset(2000000)):  \n",
        "  loc = item[0]\n",
        "  correct = sw_bound[0] <= loc[0] <= ne_bound[0] and sw_bound[1] <= loc[1] <= ne_bound[1] \n",
        "  if correct:\n",
        "    temp = clean_word(item[1])\n",
        "    if temp is not None:\n",
        "      datasets.append((item[0],temp,i))\n",
        "  if len(datasets) >= 10000:\n",
        "    break"
      ],
      "id": "7anEzodeUnI-"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ce861a33-73e7-43c1-a367-2104a866e626"
      },
      "source": [
        "## 2. [M][10 points] Metric which accepts 2 dataset items and returns distance"
      ],
      "id": "ce861a33-73e7-43c1-a367-2104a866e626"
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "21d231a0-874b-4305-a8b3-a568e653603e"
      },
      "outputs": [],
      "source": [
        "from scipy.spatial.distance import cosine, euclidean\n",
        "\n",
        "# example signature\n",
        "def my_dist(a, b) -> float:\n",
        "    ap, bp = a[0], b[0]\n",
        "    atxt, btxt = a[1], b[1]\n",
        "    return cosine(atxt, btxt) + euclidean(ap, bp)"
      ],
      "id": "21d231a0-874b-4305-a8b3-a568e653603e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7a1820d2-cf72-45b5-8242-7e093ce98417"
      },
      "source": [
        "## 3. [M][30 points] Build an index"
      ],
      "id": "7a1820d2-cf72-45b5-8242-7e093ce98417"
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use this index implementation https://github.com/IUCVLab/proximity-cut/blob/master/modules/nsw/nsw.py. We have to download the file and reference it in the next line of code to use it."
      ],
      "metadata": {
        "id": "s1xlfR8vhWVo"
      },
      "id": "s1xlfR8vhWVo"
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IwrDnKdwJGAF",
        "outputId": "65016073-8144-46ae-fee9-3bb1a2daa059"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Module NSW launched as program.\n"
          ]
        }
      ],
      "source": [
        "execfile('nsw.py')"
      ],
      "id": "IwrDnKdwJGAF"
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "-M8sIHBMKcZ7"
      },
      "outputs": [],
      "source": [
        "from nsw import Node, NSWGraph"
      ],
      "id": "-M8sIHBMKcZ7"
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "ZoSiMTNeKtC6"
      },
      "outputs": [],
      "source": [
        "def get_index():\n",
        "  G = NSWGraph([], my_dist)\n",
        "  G.build_navigable_graph([(row, 0) for row in datasets])\n",
        "  return G"
      ],
      "id": "ZoSiMTNeKtC6"
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "bfe71b69-4d53-4654-8b1f-65f18e4b7804"
      },
      "outputs": [],
      "source": [
        "index = get_index()"
      ],
      "id": "bfe71b69-4d53-4654-8b1f-65f18e4b7804"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7483bec-adec-4ef4-b192-946308db8af3"
      },
      "source": [
        "# 4. [M][10 points] write and pass some test"
      ],
      "id": "d7483bec-adec-4ef4-b192-946308db8af3"
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "f3392e3f-7a47-4ff9-9f4c-fd78e0789584"
      },
      "outputs": [],
      "source": [
        "def find(town, query, index):\n",
        "  try: \n",
        "    ne_bound, sw_bound = get_town_range_coordinates('New york', my_google_maps_api_key)\n",
        "    ce_bound = (ne_bound[0]+ sw_bound[0])/2,(ne_bound[1]+ sw_bound[1])/2\n",
        "    det = index.multi_search((ce_bound, clean_word(query)))\n",
        "    return dataset_get([datasets[i][2] for i in det])\n",
        "  except:\n",
        "    return 'Item not found'"
      ],
      "id": "f3392e3f-7a47-4ff9-9f4c-fd78e0789584"
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "514e198d-2ad2-4ee2-82f0-e1b5015b30ec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1acf0a33-af36-45b5-b8fa-5e2ccb8a3765"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[([-74.093457, 40.70621],\n",
              "  'Corner Coffee Shop, Food & Beverages, Candy. US, Jersey City, 129 Sterling Ave'),\n",
              " ([-74.026449, 40.752176],\n",
              "  'Hollywood Deli & Grocery, Food & Beverages, Groceries & Convenience Stores. US, Hoboken, 1212 Washington St'),\n",
              " ([-73.962669, 40.634539],\n",
              "  'Plaza Fruit & Vegetable Inc, Food & Beverages, Fruits & Vegetables. US, Brooklyn, 4 Newkirk Plz'),\n",
              " ([-74.033354, 40.618066],\n",
              "  'Hardes Wine & Liquor, Food & Beverages, Liquor & Beverages. US, Brooklyn, 9314 3rd Ave'),\n",
              " ([-74.032083, 40.739785],\n",
              "  'Sunshine Grocery Store Inc, Food & Beverages, Groceries & Convenience Stores. US, Hoboken, 240 Garden St')]"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "find(\"Manhattan\", \"coffee\", index)"
      ],
      "id": "514e198d-2ad2-4ee2-82f0-e1b5015b30ec"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ai",
      "language": "python",
      "name": "ai"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}